vllmImage: quay.io/modh/vllm:rhoai-2.23-cuda

# Application configuration
app: eligibility-lsd
namespace: eligibility-mcp-llamastack
partOf: eligibility-mcp-llamastack

# lsdName: rh-dev
lsdImage: registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:v2.23
lsdBaseDir: /opt/app-root/src
lsdPort: 8321

lsdEmbeddingModel: jina-embeddings-v3-gpu
lsdEmbeddingDimension: 1024
lsdEmbeddingModelProvider: jina-embeddings-v3-gpu
lsdChunkSizeInTokens: 256

lsdPlaygroundImage: quay.io/rh-aiservices-bu/llama-stack-playground:0.2.11

llamaStackDemoCoreImage: quay.io/atarazana/rag-loader:0.1.1
loaderDelay: 10
loaderJobRetries: 30

milvusDbPath: /tmp/milvus.db
milvusUri: http://vectordb-milvus.rag-base:19530
milvusToken: root:Milvus

fmsOchestratorUrl: http://localhost:8080

docsFolder: /data/docs

models:
  - name: granite-3-3-8b
    displayName: Granite 3.3 8B
    id: ibm-granite/granite-3.3-8b-instruct
    image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
    maxModelLen: '15000'
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.25-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      nodeSelector:
        group: 'llama-stack-demo'
      # productName: NVIDIA-A10G
    args:
      - '--enable-auto-tool-choice'
      - '--tool-call-parser'
      - 'granite'
      - '--chat-template'
      - '/app/data/template/tool_chat_template_granite.jinja'
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    id: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '15000' # vllm max model len
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.25-cuda
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      nodeSelector:
        group: 'llama-stack-demo'
      # productName: NVIDIA-A10G-SHARED
      # productName: T4-SHARED
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /app/data/template/tool_chat_template_llama3.1_json.jinja

  - name: jina-embeddings-v3-gpu # Matryoshka model run with vllm
    displayName: Jina Embeddings v3 GPU
    model: jinaai/jina-embeddings-v3
    image: quay.io/atarazana/modelcar-catalog:jina-embeddings-v3
    embeddingsModel: true
    maxModelLen: '8192'
    vectorDimension: 1024
    externalAccess: true
    runtime:
      templateName: vllm-serving-template
      templateDisplayName: vLLM Serving Template
      image: quay.io/modh/vllm:rhoai-2.25-cuda
      resources:
        limits:
          cpu: '2'
          memory: 8Gi
        requests:
          cpu: '1'
          memory: 4Gi
    accelerator:
      type: 'nvidia.com/gpu'
      max: '1'
      min: '1'
      nodeSelector:
        group: 'llama-stack-demo'
        modelType: 'embeddings'
    env:
      - name: HF_HUB_OFFLINE
        value: '0'

mcpServers:
  - id: mcp::eligibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/eligibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/eligibility-engine-mcp-rs:1.1.4
    mcp_transport: "sse"
    protocol: "http"
    host: eligibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"
  - id: mcp::compatibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/compatibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/compatibility-engine-mcp-rs:1.2.0
    mcp_transport: "sse"
    protocol: "http"
    host: compatibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"
  # - id: mcp::cluster-insights
  #   provider_id: model-context-protocol
  #   runtime: rust
  #   vcs:
  #     uri: https://github.com/alpha-hack-program/cluster-insights-mcp-rs.git
  #     ref: main
  #     path: .
  #   image: quay.io/atarazana/cluster-insights-mcp-rs:1.2.1
  #   mcp_transport: "sse"
  #   protocol: "http"
  #   host: cluster-insights
  #   port: 8000
  #   uri: "/sse"
  #   replicas: 1
  #   resources:
  #     limits:
  #       cpu: 500m
  #       memory: 128Mi
  #     requests:
  #       cpu: 250m
  #       memory: 64Mi
  #   env:
  #     - name: RUST_LOG
  #       value: "debug"
  #   rbac:
  #     clusterWide: true
  #     rules:
  #       # Permissions to read node information for cluster capacity analysis
  #       - apiGroups: [""]
  #         resources: ["nodes"]
  #         verbs: ["get", "list"]
  #       # Permissions to read pod information for resource allocation analysis
  #       - apiGroups: [""]
  #         resources: ["pods"]
  #         verbs: ["get", "list"]
  #       # Permissions to read namespace information for namespace-level resource usage
  #       - apiGroups: [""]
  #         resources: ["namespaces"]
  #         verbs: ["get", "list"]