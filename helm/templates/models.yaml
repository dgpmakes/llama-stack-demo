{{- $appName := .Values.app }}

{{- range .Values.models }}
{{- if not .url }}
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
    opendatahub.io/model-type: generative
    openshift.io/display-name: "{{ .displayName }}"
    security.opendatahub.io/enable-auth: '{{ default false .enableAuth }}'
    serving.kserve.io/deploymentMode: RawDeployment
    serving.knative.openshift.io/enablePassthrough: 'true'
    opendatahub.io/hardware-profile-namespace: redhat-ods-applications
    opendatahub.io/hardware-profile-name: nvidia
    opendatahub.io/connections: {{ .name }}
    opendatahub.io/model-type: generative
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"{{ $appName }}"}]'
  name: {{ .name }}
  labels:
    networking.kserve.io/visibility: exposed
    opendatahub.io/dashboard: 'true'
    opendatahub.io/genai-asset: 'true'
    monitoring.opendatahub.io/scrape: 'true'
    app.kubernetes.io/part-of: {{ $.Values.partOf }}
    {{- if .additionalLabels }}
    {{- range $key, $value := .additionalLabels }}
    {{ $key }}: "{{ $value }}"
    {{- end }}
    {{- end }}
spec:
  predictor:
    maxReplicas: {{ default 1 .maxReplicas }}
    minReplicas: 1
    model:
      modelFormat:
        name: {{ default "vLLM" .format }}
      name: ''
      runtime: {{ .name }}
      {{- /* If model.image is set, it will override the connection, else it will use the connection */ -}}
      {{- if .image }}
      storageUri: 'oci://{{ .image }}'
      {{- else }}
      storage:
        key: aws-connection-{{ .connection.name }}
        path: "{{ printf "%s/%s" .root .id }}"
      {{- end }}
      {{- if or .runtime .accelerator }}
      resources:
        {{- /* If any or accelator max or limits are set, then the limits will be set accordingly */ -}}
        {{- if or (and .accelerator .accelerator.max) (and .runtime .runtime.resources .runtime.resources.limits) }}
        limits:
          {{- if and .accelerator .accelerator.max }}    
          {{ .accelerator.type }}: '{{ .accelerator.max }}'
          {{- end }}
          {{- if and .runtime .runtime.resources .runtime.resources.limits .runtime.resources.limits.cpu }}
          cpu: '{{ .runtime.resources.limits.cpu }}'
          {{- end }}
          {{- if and .runtime .runtime.resources .runtime.resources.limits .runtime.resources.limits.memory }}
          memory: '{{ .runtime.resources.limits.memory }}'
          {{- end }}
        {{- end }}
        {{- if and .runtime .runtime.resources .runtime.resources.requests }}
        requests:
          {{- /* If any or accelator min or requests are set, then the requests will be set accordingly */ -}}
          {{- if or (and .accelerator .accelerator.min) (and .runtime .runtime.resources .runtime.resources.requests .runtime.resources.requests.cpu) (and .runtime .runtime.resources .runtime.resources.requests .runtime.resources.requests.memory) }}
          {{- if and .accelerator .accelerator.min }}
          {{ .accelerator.type }}: '{{ .accelerator.min }}'
          {{- end }}
          {{- if and .runtime .runtime.resources .runtime.resources.requests .runtime.resources.requests.cpu }}
          cpu: '{{ .runtime.resources.requests.cpu }}'
          {{- end }}
          {{- if and .runtime .runtime.resources .runtime.resources.requests .runtime.resources.requests.memory }}
          memory: '{{ .runtime.resources.requests.memory }}'
          {{- end }}
          {{- end }}
        {{- end }}
      {{- end }}
    {{- if and .accelerator .accelerator.min }}
    tolerations:
      - effect: NoSchedule
        key: {{ .accelerator.type }}
        operator: Exists
    {{- end }}
    {{- with .accelerator }}
    {{- if .productName }}
    nodeSelector:
      {{ .type }}.product: {{ .productName }}
    {{- else if .nodeSelector }}
    nodeSelector:
      {{- range $key, $value := .nodeSelector }}
      {{ $key }}: {{ $value }}
      {{- end }}
    {{- end }}
    {{- end }}
{{- if and (default true .rawDeployment) .externalAccess }}
---
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: {{ .name }}-predictor
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  to:
    kind: Service
    name: {{ .name }}-predictor
    weight: 100
  port:
    targetPort: 8080
  tls:
    termination: edge
    insecureEdgeTerminationPolicy: Redirect
{{- end }}
---
apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    argocd.argoproj.io/sync-wave: "1"
    opendatahub.io/runtime-version: {{ .runtime.version }}
    opendatahub.io/accelerator-name: migrated-gpu
    opendatahub.io/apiProtocol: {{ default "REST" .apiProtocol }}
    {{- if and .runtime .runtime.templateDisplayName }}
    opendatahub.io/template-display-name: "{{ .runtime.templateDisplayName }}"
    {{- end }}
    {{- if and .runtime .runtime.templateName }}
    opendatahub.io/template-name: "{{ .runtime.templateName }}"
    {{- end }}
    openshift.io/display-name: "{{ .displayName }}"
    {{- if and .accelerator .accelerator.type }}
    opendatahub.io/recommended-accelerators: '["{{ .accelerator.type }}"]'
    {{- end }}
    app.openshift.io/connects-to: '[{"apiVersion":"apps/v1","kind":"Deployment","name":"{{ $appName }}"}]'
  name: {{ .name }}
  labels:
    opendatahub.io/dashboard: 'true'
spec:
  annotations:
    opendatahub.io/kserve-runtime: vllm
    prometheus.io/path: /metrics
    prometheus.io/port: '8080'
  containers:
    - args:
        - '--port=8080'
        - '--model=/mnt/models'
        - '--served-model-name={{ "{{.Name}}" }}'
        {{- if .maxModelLen }}
        - '--max-model-len'
        - '{{ .maxModelLen }}'
        {{- end }}
        {{- /* If productName contains T4, then use float16 and tensor-parallel-size */ -}}
        {{- with .accelerator }}
        {{- if .productName | default "" | regexMatch "T4" }}
        - '--dtype'
        - float16
        {{- end }}
        - '--tensor-parallel-size={{ .max | default 1 }}'
        {{- end }}
        - '--distributed-executor-backend=mp'
        {{- if .embeddingsModel }}
        - '--trust-remote-code'
        {{- end }}
        {{- /* https://docs.vllm.ai/en/latest/features/tool_calling.html#ibm-granite */ -}}
        {{- if .toolCallParser }}
        - '--tool-call-parser'
        - '{{ .toolCallParser }}'
        {{- end }}
        {{- if .chatTemplate }}
        - '--chat-template'
        - '{{ .chatTemplate }}'
        {{- end }}
        {{- if .args }}
        {{- range .args }}
        - '{{ . }}'
        {{- end }}
        {{- end }}
      command:
        - python
        - '-m'
        - vllm.entrypoints.openai.api_server
      env:
        - name: HF_HOME
          value: /tmp/hf_home
        - name: VLLM_CACHE_ROOT
          value: /tmp
        {{- if .env }}
        {{- range .env }}
        - name: {{ .name }}
          value: {{ .value | default ""  | quote }}
        {{- end }}
        {{- end }}
      {{- if and .runtime .runtime.image }}
      image: '{{ .runtime.image }}'
      {{- else }}
      # Default image if runtime.image is not specified
      image: 'quay.io/modh/vllm:latest'
      {{- end }}
      name: kserve-container
      ports:
        - containerPort: 8080
          protocol: TCP
      volumeMounts:
        - mountPath: /dev/shm
          name: shm
      readinessProbe:
        tcpSocket:
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 30
        timeoutSeconds: 5
        successThreshold: 1
        failureThreshold: 3
  multiModel: false
  supportedModelFormats:
    - autoSelect: true
      name: {{ default "vLLM" .format }}
  volumes:
    - emptyDir:
        medium: Memory
        sizeLimit: {{ default "2Gi" .shmSizeLimit }}
      name: shm
{{- end }}
{{- end }}