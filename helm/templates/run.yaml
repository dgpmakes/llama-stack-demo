apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ .Values.app }}-run
data:
  run.yaml: |
    version: '2'
    image_name: rh
    storage:
      backends:
        kv_backend:
          type: kv_sqlite
          db_path: /opt/app-root/src/.llama/distributions/rh/kv_metadata.db
        sql_backend:
          type: sql_sqlite
          db_path: /opt/app-root/src/.llama/distributions/rh/sql_conversations.db
      stores:
        metadata:
          backend: kv_backend
          namespace: llama_stack
          table_name: metadata_table
        inference:
          backend: sql_backend
          namespace: llama_stack_inference
          table_name: inference_table
        conversations:
          backend: sql_backend
          namespace: llama_stack_conversations
          table_name: conversations_table
    apis:
    - agents
    - datasetio
    - inference
    - safety
    - scoring
    - tool_runtime
    - vector_io
    - files
    providers:
      inference:
      {{- range .Values.models }}
      - provider_id: {{ .name | replace " " "-" | lower }}
        provider_type: remote::vllm
        config:
          url: ${env.{{ include "rag-lsd.envVarName" .name }}_URL}
          model: ${env.{{ include "rag-lsd.envVarName" .name }}_MODEL}
          {{- if .api_token }}
          api_token: ${env.{{ include "rag-lsd.envVarName" .name }}_API_TOKEN:=fake}
          {{- end }}
          {{- if .tls_verify }}
          tls_verify: ${env.{{ include "rag-lsd.envVarName" .name }}_TLS_VERIFY:false}
          {{- end }}
          {{- if .max_tokens }}
          max_tokens: ${env.{{ include "rag-lsd.envVarName" .name }}_MAX_TOKENS:12000}
          {{- end }}
      {{- end }}
      - provider_id: ${env.AWS_ACCESS_KEY_ID:+bedrock}
        provider_type: remote::bedrock
        config:
          aws_access_key_id: ${env.AWS_ACCESS_KEY_ID:=}
          aws_secret_access_key: ${env.AWS_SECRET_ACCESS_KEY:=}
          aws_session_token: ${env.AWS_SESSION_TOKEN:=}
          region_name: ${env.AWS_DEFAULT_REGION:=}
          profile_name: ${env.AWS_PROFILE:=}
          total_max_attempts: ${env.AWS_MAX_ATTEMPTS:=}
          retry_mode: ${env.AWS_RETRY_MODE:=}
          connect_timeout: ${env.AWS_CONNECT_TIMEOUT:=60}
          read_timeout: ${env.AWS_READ_TIMEOUT:=60}
          session_ttl: ${env.AWS_SESSION_TTL:=3600}
      - provider_id: sentence-transformers
        provider_type: inline::sentence-transformers
        config: {}
      - provider_id: ${env.WATSONX_API_KEY:+watsonx}
        provider_type: remote::watsonx
        config:
          url: ${env.WATSONX_BASE_URL:=https://us-south.ml.cloud.ibm.com}
          api_key: ${env.WATSONX_API_KEY:=}
          project_id: ${env.WATSONX_PROJECT_ID:=}
      - provider_id: ${env.AZURE_API_KEY:+azure}
        provider_type: remote::azure
        config:
          api_key: ${env.AZURE_API_KEY:=}
          api_base: ${env.AZURE_API_BASE:=}
          api_version: ${env.AZURE_API_VERSION:=}
          api_type: ${env.AZURE_API_TYPE:=}
      - provider_id: ${env.VERTEX_AI_PROJECT:+vertexai}
        provider_type: remote::vertexai
        config:
          project: ${env.VERTEX_AI_PROJECT:=}
          location: ${env.VERTEX_AI_LOCATION:=us-central1}
      - provider_id: ${env.OPENAI_API_KEY:+openai}
        provider_type: remote::openai
        config:
          api_key: ${env.OPENAI_API_KEY:=}
          base_url: ${env.OPENAI_BASE_URL:=https://api.openai.com/v1}
      vector_io:
      - provider_id: milvus
        provider_type: inline::milvus
        config:
          db_path: /opt/app-root/src/.llama/distributions/rh/milvus.db
          persistence:
            backend: kv_backend
            namespace: milvus_persistence
            table_name: milvus_persistence_table
          kvstore:
            type: sqlite
            namespace: null
            db_path: /opt/app-root/src/.llama/distributions/rh/milvus_registry.db
      - provider_id: ${env.ENABLE_FAISS:+faiss}
        provider_type: inline::faiss
        config:
          kvstore:
            type: ${env.FAISS_KVSTORE_TYPE:=sqlite}
            namespace: ${env.FAISS_KVSTORE_NAMESPACE:=}
            db_path: ${env.FAISS_KVSTORE_DB_PATH:=}
            host: ${env.FAISS_KVSTORE_HOST:=}
            port: ${env.FAISS_KVSTORE_PORT:=}
            db: ${env.FAISS_KVSTORE_DB:=}
            user: ${env.FAISS_KVSTORE_USER:=}
            password: ${env.FAISS_KVSTORE_PASSWORD:=}
            ssl_mode: ${env.FAISS_KVSTORE_SSL_MODE:=}
            ca_cert_path: ${env.FAISS_KVSTORE_CA_CERT_PATH:=}
            table_name: ${env.FAISS_KVSTORE_TABLE_NAME:=}
            collection_name: ${env.FAISS_KVSTORE_COLLECTION_NAME:=}
      - provider_id: ${env.MILVUS_ENDPOINT:+milvus-remote}
        provider_type: remote::milvus
        config:
          uri: ${env.MILVUS_ENDPOINT:=}
          token: ${env.MILVUS_TOKEN:=}
          secure: ${env.MILVUS_SECURE:=}
          consistency_level: ${env.MILVUS_CONSISTENCY_LEVEL:=}
          ca_pem_path: ${env.MILVUS_CA_PEM_PATH:=}
          client_pem_path: ${env.MILVUS_CLIENT_PEM_PATH:=}
          client_key_path: ${env.MILVUS_CLIENT_KEY_PATH:=}
          kvstore:
            type: sqlite
            namespace: null
            db_path: {{ .Values.lsdBaseDir }}/.llama/distributions/rh/milvus_remote_registry.db
      safety:
      - provider_id: trustyai_fms
        provider_type: remote::trustyai_fms
        module: llama_stack_provider_trustyai_fms==0.2.2
        config:
          orchestrator_url: ${env.FMS_ORCHESTRATOR_URL:=}
          ssl_cert_path: ${env.FMS_SSL_CERT_PATH:=}
          shields: {}
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence:
            agent_state:
              backend: kv_backend
              namespace: agents_state
              table_name: agents_state_table
            responses:
              backend: sql_backend
              namespace: agents_responses
              table_name: agents_responses_table
      eval:
      - provider_id: trustyai_lmeval
        provider_type: remote::trustyai_lmeval
        module: llama_stack_provider_lmeval==0.2.4
        config:
          use_k8s: ${env.TRUSTYAI_LMEVAL_USE_K8S:=true}
          base_url: ${env.VLLM_URL:=http://localhost:8000/v1}
      - provider_id: ${env.EMBEDDING_MODEL:+trustyai_ragas_inline}
        provider_type: inline::trustyai_ragas
        module: llama_stack_provider_ragas.inline
        config:
          embedding_model: ${env.EMBEDDING_MODEL:=}
      - provider_id: ${env.KUBEFLOW_LLAMA_STACK_URL:+trustyai_ragas_remote}
        provider_type: remote::trustyai_ragas
        module: llama_stack_provider_ragas.remote
        config:
          embedding_model: ${env.EMBEDDING_MODEL:=}
          kubeflow_config:
            results_s3_prefix: ${env.KUBEFLOW_RESULTS_S3_PREFIX:=}
            s3_credentials_secret_name: ${env.KUBEFLOW_S3_CREDENTIALS_SECRET_NAME:=}
            pipelines_endpoint: ${env.KUBEFLOW_PIPELINES_ENDPOINT:=}
            namespace: ${env.KUBEFLOW_NAMESPACE:=}
            llama_stack_url: ${env.KUBEFLOW_LLAMA_STACK_URL:=}
            base_image: ${env.KUBEFLOW_BASE_IMAGE:=}
            pipelines_api_token: ${env.KUBEFLOW_PIPELINES_TOKEN:=}
      datasetio:
      - provider_id: huggingface
        provider_type: remote::huggingface
        config:
          kvstore:
            backend: kv_backend
            namespace: huggingface_datasetio
            table_name: hf_datasetio_table 
      - provider_id: localfs
        provider_type: inline::localfs
        config:
          kvstore:
            backend: kv_backend
            namespace: localfs_datasetio
            table_name: localfs_datasetio_table      
      scoring:
      - provider_id: basic
        provider_type: inline::basic
        config: {}
      - provider_id: llm-as-judge
        provider_type: inline::llm-as-judge
        config: {}
      - provider_id: braintrust
        provider_type: inline::braintrust
        config:
          openai_api_key: ${env.OPENAI_API_KEY:=}
      tool_runtime:
      {{- if .Values.tavilySecretName }}
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_SEARCH_API_KEY:=}
          max_results: 3
      {{- end }}
      - provider_id: rag-runtime
        provider_type: inline::rag-runtime
        config: {}
      - provider_id: model-context-protocol
        provider_type: remote::model-context-protocol
        config: {}
      files:
      - provider_id: meta-reference-files
        provider_type: inline::localfs
        config:
          storage_dir: /opt/app-root/src/.llama/distributions/rh/files
          metadata_store:
            backend: sql_backend
            namespace: files_metadata
            table_name: files_metadata_table
    metadata_store:
      type: sqlite
      db_path: {{ .Values.lsdBaseDir }}/.llama/distributions/rh/registry.db
    inference_store:
      type: sqlite
      db_path: {{ .Values.lsdBaseDir }}/.llama/distributions/rh/inference_store.db
    models:
    {{- range .Values.models }}
    - metadata:
        {{- if .embeddingsModel }}
        embedding_dimension: {{ .vectorDimension | default $.Values.lsdEmbeddingDimension | default 768 }}
        {{- else }}
        {}
        {{- end }}
      model_id: {{ .name | replace " " "-" | lower }}
      {{- /* Determine if the model is handled by the internal sentence-transformers provider */ -}}
      {{- $isInternalEmbedding := and .connection (eq .connection.name "sentence-transformers") }}
      provider_id: {{ if $isInternalEmbedding }}"sentence-transformers"{{ else }}{{ .name | replace " " "-" | lower }}{{ end }}
      {{- if $isInternalEmbedding }}
      # This tells the provider which Hugging Face repo to download
      provider_model_id: {{ .id | quote }}
      {{- end }}
      model_type: {{ if .embeddingsModel }}embedding{{ else }}llm{{ end }}
    {{- end }}
    shields: []
    vector_dbs: []
    datasets: []
    scoring_fns: []
    benchmarks: []
    tool_groups:
    {{- if .Values.tavilySecretName }}
    - toolgroup_id: builtin::websearch
      provider_id: tavily-search
    {{- end }}
    - toolgroup_id: builtin::rag
      provider_id: rag-runtime
    {{- range .Values.mcpServers }}
    - toolgroup_id: mcp::{{ .id }}
      provider_id: {{ .provider_id }}
      mcp_endpoint:
        uri: {{ .protocol }}://{{ .host }}.{{ $.Release.Namespace }}.svc:{{ .port }}{{ .uri }}
    {{- end }}
    registered_resources:
      tool_groups:
      {{- if .Values.tavilySecretName }}
      - toolgroup_id: builtin::websearch
        provider_id: tavily-search
      {{- end }}
      - toolgroup_id: builtin::rag
        provider_id: rag-runtime
      {{- range .Values.mcpServers }}
      - toolgroup_id: mcp::{{ .id }}
        provider_id: {{ .provider_id }}
        mcp_endpoint:
          uri: {{ .protocol }}://{{ .host }}.{{ $.Release.Namespace }}.svc:{{ .port }}{{ .uri }}
      {{- end }}
    server:
      port: 8321