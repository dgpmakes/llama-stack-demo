# ---
# apiVersion: llamastack.io/v1alpha1
# kind: LlamaStackDistribution
# metadata:
#   annotations:
#     openshift.io/display-name: lsd-genai-playground
#   name: lsd-genai-playground
#   labels:
#     opendatahub.io/dashboard: 'true'
# spec:
#   replicas: 1
#   server:
#     containerSpec:
#       command:
#         - /bin/sh
#         - '-c'
#         - llama stack run /etc/llama-stack/run.yaml
#       env:
#         - name: VLLM_TLS_VERIFY
#           value: 'false'
#         - name: MILVUS_DB_PATH
#           value: ~/.llama/milvus.db
#         - name: FMS_ORCHESTRATOR_URL
#           value: 'http://localhost'
#         - name: VLLM_MAX_TOKENS
#           value: '4096'
#         - name: VLLM_API_TOKEN_1
#           value: fake
#         - name: LLAMA_STACK_CONFIG_DIR
#           value: /opt/app-root/src/.llama/distributions/rh/
#       name: llama-stack
#       port: 8321
#       resources:
#         limits:
#           cpu: '2'
#           memory: 12Gi
#         requests:
#           cpu: 250m
#           memory: 500Mi
#     distribution:
#       name: rh-dev
#     userConfig:
#       configMapName: llama-stack-config
# ---
# kind: ConfigMap
# apiVersion: v1
# metadata:
#   name: llama-stack-config
#   labels:
#     llamastack.io/distribution: lsd-genai-playground
#     opendatahub.io/dashboard: 'true'
# data:
#   run.yaml: |
#     # Llama Stack Configuration
#     version: "2"
#     image_name: rh
#     apis:
#     - agents
#     - datasetio
#     - files
#     - inference
#     - safety
#     - scoring
#     - tool_runtime
#     - vector_io
#     providers:
#       inference:
#       - provider_id: sentence-transformers
#         provider_type: inline::sentence-transformers
#         config: {}
#       # - provider_id: vllm-inference-1
#       #   provider_type: remote::vllm
#       #   config:
#       #     api_token: ${env.VLLM_API_TOKEN_1:=fake}
#       #     max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
#       #     tls_verify: ${env.VLLM_TLS_VERIFY:=true}
#       #     url: http://llama-3-1-8b-w4a16-predictor.llama-stack-demo.svc.cluster.local:8080/v1
#       {{- range .Values.models }}
#       - provider_id: {{ .name | replace " " "-" | lower }}
#         provider_type: remote::vllm
#         config:
#           url: http://{{ .name }}-predictor.{{ .namespace }}.svc.cluster.local:8080/v1
#           api_token: ${env.VLLM_API_TOKEN_1:=fake}
#           max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
#           tls_verify: ${env.VLLM_TLS_VERIFY:=true}
#       {{- end }}
#       vector_io:
#       - provider_id: milvus
#         provider_type: inline::milvus
#         config:
#           db_path: /opt/app-root/src/.llama/distributions/rh/milvus.db
#           kvstore:
#             db_path: /opt/app-root/src/.llama/distributions/rh/milvus_registry.db
#             namespace: null
#             type: sqlite
#       agents:
#       - provider_id: meta-reference
#         provider_type: inline::meta-reference
#         config:
#           persistence_store:
#             db_path: /opt/app-root/src/.llama/distributions/rh/agents_store.db
#             namespace: null
#             type: sqlite
#           responses_store:
#             db_path: /opt/app-root/src/.llama/distributions/rh/responses_store.db
#             type: sqlite
#       eval: []
#       files:
#       - provider_id: meta-reference-files
#         provider_type: inline::localfs
#         config:
#           metadata_store:
#             db_path: /opt/app-root/src/.llama/distributions/rh/files_metadata.db
#             type: sqlite
#           storage_dir: /opt/app-root/src/.llama/distributions/rh/files
#       datasetio:
#       - provider_id: huggingface
#         provider_type: remote::huggingface
#         config:
#           kvstore:
#             db_path: /opt/app-root/src/.llama/distributions/rh/huggingface_datasetio.db
#             namespace: null
#             type: sqlite
#       scoring:
#       - provider_id: basic
#         provider_type: inline::basic
#         config: {}
#       - provider_id: llm-as-judge
#         provider_type: inline::llm-as-judge
#         config: {}
#       tool_runtime:
#       - provider_id: rag-runtime
#         provider_type: inline::rag-runtime
#         config: {}
#       - provider_id: model-context-protocol
#         provider_type: remote::model-context-protocol
#         config: {}
#     metadata_store:
#       type: sqlite
#       db_path: /opt/app-root/src/.llama/distributions/rh/inference_store.db
#     models:
#     - provider_id: sentence-transformers
#       model_id: granite-embedding-125m
#       provider_model_id: ibm-granite/granite-embedding-125m-english
#       model_type: embedding
#       metadata:
#         embedding_dimension: 768
#     # - provider_id: vllm-inference-1
#     #   model_id: llama-3-1-8b-w4a16
#     #   model_type: llm
#     #   metadata:
#     #     display_name: Llama 3.1 8B
#     {{- range .Values.models }}
#     - provider_id: {{ .name | replace " " "-" | lower }}
#       model_id: {{ .name | replace " " "-" | lower }}
#       model_type: llm
#       metadata:
#         display_name: {{ .displayName }}
#     {{- end }}
#     shields: []
#     vector_dbs: []
#     datasets: []
#     scoring_fns: []
#     benchmarks: []
#     tool_groups:
#     - toolgroup_id: builtin::rag
#       provider_id: rag-runtime
#     server:
#       port: 8321

