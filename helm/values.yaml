# Application configuration
app: llama-stack-demo
partOf: llama-stack-demo

lsdName: rh-dev
# lsdImage: registry.redhat.io/rhoai/odh-llama-stack-core-rhel9:v2.23
lsdBaseDir: /opt/app-root/src
lsdPort: 8321

# Route timeout for HAProxy (default 30s is too short for model loading)
routeTimeout: 1m

# lsdEmbeddingModel: granite-embedding-125m
lsdEmbeddingModel: nomic-embed-text-v1.5
lsdEmbeddingDimension: 768
lsdEmbeddingModelProvider: sentence-transformers
lsdChunkSizeInTokens: 800
lsdChunkOverlapInTokens: 400
lsdVectorStoreProviderId: milvus
lsdVectorStoreName: rag-store
lsdRanker: default
lsdScoreThreshold: 0.7
lsdMaxNumResults: 10
lsdTestQuery: Tell me about taxes in Lysmark.

llamaStackDemoCoreImage: quay.io/atarazana/llama-stack-demo-core:0.5.3

loaderDelay: 15
loaderJobRetries: 60

milvusDbPath: /tmp/milvus.db
fmsOchestratorUrl: http://localhost:8080

docsFolder: /data/docs

models:
  # - name: granite-3-3-8b
  #   displayName: Granite 3.3 8B
  #   id: ibm-granite/granite-3.3-8b-instruct
  #   image: quay.io/redhat-ai-services/modelcar-catalog:granite-3.3-8b-instruct
  #   maxModelLen: '15000'
  #   maxTokens: '4096' # llama stack vllm max token, default is 4096
  #   tlsVerify: false
  #   externalAccess: true
  #   runtime:
  #     templateName: vllm-serving-template
  #     templateDisplayName: vLLM Serving Template
  #     version: v0.9.1.0
  #     # image: quay.io/modh/vllm:rhoai-2.25-cuda
  #     image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768
  #     resources:
  #       limits:
  #         cpu: '8'
  #         memory: 24Gi
  #       requests:
  #         cpu: '6'
  #         memory: 24Gi
  #   accelerator:
  #     type: nvidia.com/gpu
  #     max: '1'
  #     min: '1'
  #     nodeSelector:
  #       group: 'llama-stack-demo'
  #     # productName: NVIDIA-A10G
  #   args:
  #     - '--enable-auto-tool-choice'
  #     - '--tool-call-parser'
  #     - 'granite'
  #     - '--chat-template'
  #     - '/app/data/template/tool_chat_template_granite.jinja'
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    id: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    url: "http://llama-3-1-8b-w4a16-predictor:80/v1"
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '24000'
    maxTokens: '4096'
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-cuda-runtime
      templateDisplayName: vLLM NVIDIA GPU ServingRuntime for KServe
      version: v0.9.1.0
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9:3.2.1
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      nodeSelector:
        group: 'llama-stack-demo'
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /opt/app-root/template/tool_chat_template_llama3.1_json.jinja

mcpServers:
  - id: eligibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/eligibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/eligibility-engine-mcp-rs:1.2.1
    mcp_transport: "sse"
    protocol: "http"
    host: eligibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"
  - id: compatibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/compatibility-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/compatibility-engine-mcp-rs:1.3.3
    mcp_transport: "sse"
    protocol: "http"
    host: compatibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"
  # - id: cluster-insights
  #   provider_id: model-context-protocol
  #   runtime: rust
  #   vcs:
  #     uri: https://github.com/alpha-hack-program/cluster-insights-mcp-rs.git
  #     ref: main
  #     path: .
  #   image: quay.io/atarazana/cluster-insights-mcp-rs:1.3.2
  #   mcp_transport: "sse"
  #   protocol: "http"
  #   host: cluster-insights
  #   port: 8000
  #   uri: "/sse"
  #   replicas: 1
  #   resources:
  #     limits:
  #       cpu: 500m
  #       memory: 128Mi
  #     requests:
  #       cpu: 250m
  #       memory: 64Mi
  #   env:
  #     - name: RUST_LOG
  #       value: "debug"
  #   rbac:
  #     clusterWide: true
  #     rules:
  #       # Permissions to read node information for cluster capacity analysis
  #       - apiGroups: [""]
  #         resources: ["nodes"]
  #         verbs: ["get", "list"]
  #       # Permissions to read pod information for resource allocation analysis
  #       - apiGroups: [""]
  #         resources: ["pods"]
  #         verbs: ["get", "list"]
  #       # Permissions to read namespace information for namespace-level resource usage
  #       - apiGroups: [""]
  #         resources: ["namespaces"]
  #         verbs: ["get", "list"]
  - id: finance-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/alpha-hack-program/finance-engine-mcp-rs.git
      ref: main
      path: .
    image: quay.io/atarazana/finance-engine-mcp-rs:2.0.3
    mcp_transport: "sse"
    protocol: "http"
    host: finance-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    addAppConfig: true
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"