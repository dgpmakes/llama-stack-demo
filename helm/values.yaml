vllmImage: quay.io/modh/vllm:rhoai-2.23-cuda

# Application configuration
app: llama-stack-demo-lsd
namespace: llama-stack-demo
partOf: llama-stack-demo
ignoreHelmHooks: true #Use this to uninstall Grafana deployment automatically

lsdName: rh-dev
lsdImage: "registry.redhat.io/rhoai/odh-llama-stack-core-rhel9@sha256:a9e415ce30b0a61ae35ef43870b03ccb193074fdbd02683757f1908343266837"
lsdBaseDir: /opt/app-root/src
lsdPort: 8321

lsdEmbeddingModel: sentence-transformers/nomic-ai/nomic-embed-text-v1.5
lsdEmbeddingDimension: 768
lsdEmbeddingModelProvider: sentence-transformers
allowUnknownEmbeddingModel: true
lsdChunkSizeInTokens: 256
lsdChunkOverlapInTokens: 400
lsdRanker: default
lsdScoreThreshold: 0.7
lsdMaxNumResults: 10
lsdVectorStoreName: "rag-store"
lsdVectorStoreProviderId: milvus
lsdTestQuery: Tell me about taxes in Lysmark.
tavilySecretName: ""

llamaStackDemoCoreImage: quay.io/dgarciap/llama-stack-demo-app:0.2.2
loaderDelay: 10
loaderJobRetries: 30
lsdScoreThreshold: 0.7
lsdMaxNumResults: 10

milvusDbPath: /tmp/milvus.db
fmsOchestratorUrl: http://localhost:8080

docsFolder: /data/docs

models:
  - name: llama-3-1-8b-w4a16
    displayName: Llama 3.1 8B
    id: RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16
    image: quay.io/redhat-ai-services/modelcar-catalog:llama-3.2-8b-instruct-quantized.w4a16
    maxModelLen: '15000' # vllm max model len
    maxTokens: '4096' # llama stack vllm max token, default is 4096
    tlsVerify: false
    externalAccess: true
    runtime:
      templateName: vllm-cuda-runtime
      templateDisplayName: vLLM NVIDIA GPU ServingRuntime for KServe
      version: v0.9.1.0
      # image: quay.io/modh/vllm:rhoai-2.25-cuda
      image: registry.redhat.io/rhaiis/vllm-cuda-rhel9@sha256:ad756c01ec99a99cc7d93401c41b8d92ca96fb1ab7c5262919d818f2be4f3768
      resources:
        limits:
          cpu: '8'
          memory: 24Gi
        requests:
          cpu: '6'
          memory: 24Gi
    accelerator:
      type: nvidia.com/gpu
      max: '1'
      min: '1'
      nodeSelector:
        group: 'llama-stack-demo'
      # productName: NVIDIA-A10G-SHARED
      # productName: T4-SHARED
    env:
      - name: VLLM_SKIP_WARMUP
        value: "true"
    args:
      - --enable-auto-tool-choice
      - --tool-call-parser
      - llama3_json
      - --chat-template
      - /opt/app-root/template/tool_chat_template_llama3.1_json.jinja
  - name: nomic-embed-text-v1-5
    displayName: Nomic Embed Text v1.5
    id: nomic-ai/nomic-embed-text-v1.5
    url: "inline"
    embeddingsModel: true
    vectorDimension: 768
    connection:
      name: sentence-transformers
    runtime:
      templateName: "none"
      version: "none"
mcpServers:
  - id: compatibility-engine
    provider_id: model-context-protocol
    runtime: rust
    vcs:
      uri: https://github.com/dgpmakes/compatibility-engine.git
      ref: main
      path: .
    image: quay.io/dgarciap/compatibility-engine:latest
    mcp_transport: "sse"
    protocol: "http"
    host: compatibility-engine
    port: 8000
    uri: "/sse"
    replicas: 1
    resources:
      limits:
        cpu: 500m
        memory: 128Mi
      requests:
        cpu: 250m
        memory: 64Mi
    env:
      - name: RUST_LOG
        value: "debug"